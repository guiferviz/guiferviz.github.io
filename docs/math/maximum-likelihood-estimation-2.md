---
excerpt: >
    Ejemplo pr치ctico.
    Paso a paso a trav칠s de la derivaci칩n matem치tica necesaria para estimar
    el par치metro de una distribuci칩n de Bernoulli con MLE.
date: 2019-10-18 19:40:00
tags: [Statistics]
comments: true
jsxgraph: true
---


# 游쀯릖 Maximum Likelihood Estimation - Parte 2

En esta segunda parte vamos a dar por hecho que se conoce el concepto de MLE
que fue visto en detalle en la [primera parte][mle1] de esta serie de dos
art칤culos.

Esta segunda parte es mucho m치s intensa y emocionante que la anterior. En la
primera parte se explica el concepto de forma gr치fica y con un ejemplo
interactivo, pero una vez eso se tiene claro llega el momento de introducir
notaci칩n matem치tica 游땳.

Tal y como dijimos en el anterior at칤culo, hoy demostraremos el porqu칠 la
divisi칩n $\frac{\text{# caras}}{\text{# lanzamientos}}$ nos devuelve el valor
del par치metro $p$ m치s *likelihood* en cualquier secuencia de lanzamientos de
una moneda.


## Requisitos

* Nociones muy b치sicas de estad칤stica, como saber calcular la media de una
muestra.

* Conocer el concepto del Maximum Likelihood Estimation (MLE). Puedes
aprenderlo sin salir de este blog en la primera parte.

* Saber lo que es una derivada y c칩mo usarla para minimizar una funci칩n.


## M치s monedas

Estuvimos trabajando con monedas y seguiremos con ellas durante este art칤culo.
En esta segunda parte vamos a hacer un esfuerzo por obtener expresiones
generales, que sirven para cualquier tipo de moneda y para cualquier resultado
obtenido por dicha moneda. No os quiero asustar con palabrer칤a, vayamos paso a
paso. En primer lugar llamaremos $X$ al resultado obtenido tras lanzar $N$
monedas, es decir, $X$ es una lista con caras y cruces. La primera cara o cruz
corresponde al primer lanzamiento y as칤 sucesivamente hasta el lanzamiento $N$.

$X = (X_1, X_2, \dots, X_N)$ Diremos que $X_i = 1$ cuando haya salido cara en
el lanzamiento $i$. Del mismo modo, diremos que $X_i = 0$ cuando el lanzamiento
$i$ haya sido cruz. En ambos casos $i = 1, 2, \dots, N$. Para que esto quede
claro vamos a ver un ejemplo. Suponed que lanzamos una moneda 3 veces y
obtenemos: <span class="head"></span> , <span class="head"></span> , <span
class="tail"></span>. 쮺u치l es el vector $X$ asociado a dicha secuencia de
lanzamientos? Muy sencillo: $X = (1, 1, 0)$.

Hagamos un peque침o repaso a las probabilidades de obtener cara o cruz. 쮺u치l es
la probabilidad de obtener cara?

<p style="text-align: center" markdown>
$P($<span class="head"></span>$) = P(X_i = 1) = p$
</p>

쮺u치l es la probabilidad de obtener cruz?

<p style="text-align: center" markdown>
$P($<span class="tail"></span>$) = P(X_i = 0) = 1 - p$
</p>

A modo de informaci칩n extra, este tipo de distribuci칩n de probabilidad se le
conoce como distribuci칩n de
[*Bernoulli*](https://es.wikipedia.org/wiki/Distribuci%C3%B3n_de_Bernoulli).
Por lo tanto, en este art칤culo nos vamos a centrar en estimar el par치metro de
una distribuci칩n de *Bernoulli*, par치metro que en nuestro ejemplo con monedas
hemos llamado $p$.


## Yendo un poco m치s lejos

쮺u치l es la probabilidad de obtener $X_i$? *쮺칩mo?* Me refiero a cu치l es la
probabilidad de que en un lanzamiento $i$ (es decir, en cualquier lanzamiento)
se consiga el resultado obtenido. Posiblemente me dir칠is: pues eso depende del
resultado obtenido. No os equivoc치is. Como ya hemos dicho antes, si $X_i = 1$
entonces la probabilidad es $p$ y si $X_i = 0$ la probabilidad es $1 - p$.
Vamos a juntar eso en una sola expresi칩n:

$$P(X_i) = p^{X_i}(1 - p)^{1 - X_i}$$

En esa sola l칤nea se resume la probabilidad de cada lanzamiento. Hemos
conseguido expresar la probabilidad de obtener un lanzamiento dado sin
importarnos realmente cu치l es el resultado de dicho lanzamiento. Podemos verlo
como dos factores, por un lado $p^{X_i}$ y por otro lado $(1 - p)^{1 - X_i}$.
Cuando $X_i = 1$, entonces el segundo factor se neutraliza debido a que el
exponente pasa a valer 0 (cualquier n칰mero elevado a 0 es 1, lo que no modifica
la multiplicaci칩n). Cuando $X_i = 0$, es el primer factor el que se neutraliza.
Por lo tanto, al sustituir $X_i$ por su valor se obtiene ex치ctamente el mismo
valor de probabilidad que antes.

$$
\require{cancel}
\begin{align}
P(X_i = 1) &= p ^ 1 \cancelto{1}{(1 - p) ^ {1 - 1}} = p\\
P(X_i = 0) &= \cancelto{1}{p^{0}} (1 - p) ^ {1 - 0} = 1 - p
\end{align}
$$

Las probabilidades dependen del valor de $p$, es decir, est치n condicionadas al
valor de ese par치metro. Es por ello que muchas veces se utiliza la sintaxis
propia de las probabilidades condicionales $P(X_i\,\vert\,p)$. La forma de
escribirlo depende bastante del autor.


## Probabilidad conjunta

Ahora que sabemos cu치l es la probabilidad de cada uno de los lanzamientos
podemos hallar la probabilidad de obtener todos los lanzamientos uno detr치s de
otro, es decir $P(X\ \vert\ p)$. Sabemos que los lanzamientos son eventos
independientes, por lo que para obtener el valor de probabilidad de todos los
lanzamientos juntos basta con multiplicar la probabilidad de cada uno de los
lanzamientos (tal y como hicimos en los ejercicios de ejemplo de la [primera
parte][mle1]. Matem치ticamente podemos expresar el valor de **probabilidad
conjunta** utilizando el productorio $\prod$ de la siguiente manera

$$P(X\ \vert\ p) = \prod_{i = 1}^{N} P(X_i\ \vert\ p) = \prod_{i = 1}^{N} p^{X_i}(1 - p)^{1 - X_i}$$

**Recapitulamos**: hemos encontrado la forma de expresar en una sola ecuaci칩n
la probabilidad de obtener un lanzamiento concreto sin importarnos del
resultado obtenido en dicho lanzamiento. Ahora hemos hallado la probabilidad de
obtener un conjunto de lanzamientos. Nos fijamos que esa expresi칩n es muy
general, no nos importan ni saber cu치l es el n칰mero de lanzamientos, ni saber
los resultados de dichos lanzamientos y tampoco el valor de $p$ asociado a la
moneda utilizada. Tal y como ven칤amos comentando en el anterior art칤culo, lo
que realmente nos interesa es, dado un conjunto de lanzamientos $X$, obtener el
valor del par치metro $p$ que maximice la probabilidad de obtener dicha $X$. Para
obtener ese valor debemos de maximizar $P(X\ \vert\ p)$.


## Funci칩n *Likelihood*

Vamos a definir una funci칩n que nos de el valor de probabilidad de $X$ en
funci칩n del valor del par치metro $p$. Al maximizar dicha funci칩n obtendremos el
valor de $p$ que mayor probabilidad obtene en esos datos. Esa funci칩n se conoce
como la **funci칩n *likelihood*** y se representa como

$$L(p;\ X) = P(X\ \vert\ p) = \prod_{i = 1}^{N} P(X_i\ \vert\ p).$$

Los dos puntos separan las variables de los par치metros fijos. Los valores $X_i$
no son conocidos pero son fijos, es decir, que nosotros queremos maximizar el
valor del parametro $p$ usando unos valores de $X_i$ que no cambiar치n durante
el proceso (para m치s informaci칩n mirar esta [pregunta de
*math.stackexchange*](http://math.stackexchange.com/questions/342268/what-does-the-semicolon-mean-in-a-function-definition)).
La forma de designar esta funci칩n var칤a bastante de un autor a otro. En muchos
textos en lugar de la letra $L$ se utiliza $\mathcal{L}$ para designar a la
funci칩n *likelihood*. Personalmente prefiero reservar $\mathcal{L}$ para los
multiplicadores de *Lagrange*, pero vamos, que no es algo relevante. Otros
simplemente escriben $L(p)$ dando por hecho la existencia de $X$.

Como deber칤as de saber para entender lo que viene a continuaci칩n, a la hora de
querer maximizar una funci칩n lo m치s com칰n es utilizar la primera derivada e
igualarla a 0. Derivar $L(p;\ X)$ con ese productorio... complicado. Ser칤a
ideal poder cambiar ese productorio por otra expresi칩n mucho m치s sencilla de
derivar. Eso se consigue de una forma muy r치pida, aplicando el logaritmo a la
funci칩n *likelihood*. La funci칩n *likelihood* devuelve una probabilidad, por lo
que se encuentra acotada entre 0 y 1. Vamos a recordar la forma de la **funci칩n
logaritmo**.

<script>
    window.addEventListener('load', function() {
        JXG.Options.text.useMathJax = true;
        var board = JXG.JSXGraph.initBoard('jxgbox1', {
            boundingbox: [-1, 2.0, 5.0, -2],
            showNavigation: false,
            axis: true,
            zoom: {
                factorX: 1.25,
                factorY: 1.25,
                wheel: true,
                needshift: false,
                eps: 0.1
            },
            pan: {
                needshift: false
            }
        });
        var graph = board.create('functiongraph', [function(x) {
            return Math.log(x);
        }]);
        board.create('text',[3, 1, function() { return '\\[\\ln\\ x\\]'; }], {
            fixed: true,
            fontSize: 20
        });
        board.fullUpdate();
    });
</script>

<p id="jxgbox1" style="margin-left: auto; margin-right: auto; max-width: 100%; max-height: 100vw; text-indent: 0px; width: 500px; height: 400px; overflow: hidden; position: relative;"></p>

Se observa que se trata de una funci칩n mon칩tona creciente, es decir, que la
curva que representa la funci칩n nunca va para abajo. Para los m치s curiosos, el
"nunca va para abajo" se expresa matem치ticamente con $\forall x \le y, f(x) \le
f(y)$ (para m치s informaci칩n ver [funciones mon칩tonas en
Wikipedia](https://es.wikipedia.org/wiki/Funci%C3%B3n_mon%C3%B3tona)). Esto
quiere decir que, si aplicamos el logaritmo a la funci칩n $L$, no cambiar치 el
m치ximo de la funci칩n. Dicha transformaci칩n nos permitir치 derivar de forma m치s
sencilla y obtener el mismo m치ximo que si maximiz치ramos el productorio
original. Es importante darse cuenta de que los valores de la funci칩n
cambiar칤an (ya no estar칤an expresando la probabilidad de $X$), pero lo
importante es que el m치ximo es el mismo en las dos funciones. Tras aplicarle el
logaritmo, la funci칩n *likelihood* se suele designar con $\ell$ y se le llama
*log-likelihood*. Matem치ticamente se define como

$$\ell(p;\ X) = \ln(L(p;\ X)) = \sum_{i = 1}^{N} \ln P(X_i\ \vert\ p).$$

No solo cambiamos el productorio por un sumatorio, sino que tambi칠n eliminamos
el producto que encontr치bamos al calcular la probabilidad de una sola moneda.

$$
\begin{align}
\ln P(X_i\ \vert\ p) &= \ln p^{X_i} + \ln (1 - p)^{1 - X_i}\\
&= X_i \ln p + (1 - X_i) \ln (1 - p)
\end{align}
$$

En esa 칰ltima l칤nea hemos hecho uso de la propiedad que dice $\ln a^b = b \ln
a$. La funci칩n completa quedar칤a de la siguiente manera:

$$
\ell(p;\ X) = \sum_{i = 1}^{N} \left[ X_i \ln p + (1 - X_i) \ln (1 - p) \right].
$$

Maximicemos pues la funci칩n $\ell(p;\ X)$ igualando a 0 la primera derivada, es
decir, $\dfrac{d \ell(p)}{d p} = 0$.

$$
\require{cancel}
\begin{align}
\dfrac{d \ell(p;\ X)}{d p} &= \sum_{i = 1}^{N} \left[ \dfrac{X_i}{p} + \dfrac{1 - X_i}{1 - p} \left(\dfrac{d (1 - p)}{d p}\right) \right] = \\
&= \sum_{i = 1}^{N} \left[ \dfrac{X_i}{p} + \dfrac{1 - X_i}{1 - p} (-1) \right] = \\
&= \sum_{i = 1}^{N} \left[ \dfrac{X_i}{p} - \dfrac{1 - X_i}{1 - p} \right] = \\
&= \sum_{i = 1}^{N} \left[ (1 - p) X_i - p (1 - X_i) \right] =\\
&= \sum_{i = 1}^{N} \left[ X_i - \cancel{X_i p} - p + \cancel{X_i p} \right] = \\
&= \sum_{i = 1}^{N} \left[ X_i - p \right] = \\
&= - N p + \sum_{i = 1}^{N} X_i = 0\\
\end{align}
$$

Por si alguien se ha perdido durante la derivaci칩n aclarar칠 las 2 cosas que m치s
problemas pueden dar.

* $\dfrac{d \ln f(x)}{d x} = \dfrac{1}{f(x)} f'(x)$, siendo $f$ una funci칩n
cualquiera.

* $\sum_{i = 1}^{N} c = Nc$ siendo $c$ una constante cualquiera. En la
derivaci칩n el par치metro $p$ no cambia conforme cambiamos de lanzamiento $i$,
por lo que es una constante dentro de ese sumatorio.

Llegados a este punto solo queda despejar $p$ y obtenemos que

$$p = \dfrac{1}{N} \sum_{i = 1}^{N} X_i\text{.}$$

쯆s suena este resultado? Seguro que s칤. Vamos a escribirlo utilizando palabras
m치s amigables.

$$p = \dfrac{\text{n칰mero de caras}}{\text{n칰mero de lanzamientos}}.$$

Se trata ex치ctamente de la misma expresi칩n. El sumatorio cuenta el n칰mero de
caras puesto que $X_i = 1$ cuando el lanzamiento $i$ ha salido cara y $X_i = 0$
cuando ha sido cruz. El n칰mero de lanzamientos es $N$.

Debido a que el valor del par치metro $p$ es una estimaci칩n y no es su valor real
(no sabemos el valor real de la moneda que ha generado dichos lanzamientos) se
le suele poner un sombrerito encima $\hat{p}$. Por lo tanto finalmente tenemos
que $\hat{p} = \dfrac{1}{N} \sum_{i = 1}^{N} X_i\text{,}$ lo que traducido a
palabras significa que el valor de $p$ estimado con el MLE no es otra cosa que
el n칰mero de caras entre el n칰mero total de lanzamientos.


## 쯈u칠 hemos hecho?

A modo de resumen voy a exponer los pasos seguidos para obtener el resultado
final. En primer lugar hemos definido $X$ como una serie de lanzamientos de
una moneda. Hemos obtenido una expresi칩n que nos permite hallar la probabilidad
de un lanzamiento dado, $P(X_i\ \vert\ p)$. Multiplicando el valor de
probabilidad de cada moneda obtenemos la probabilidad conjunta de todos los
lanzamientos, $P(X\ \vert\ p)$. Hemos creado una funci칩n, *likelihood
function*, que devuelve la probabilidad conjunta en funci칩n de $p$, $L(p;\
X)$. Con el objetivo de maximizar esa funci칩n le hemos aplicado el logaritmo
obteniendo as칤 la funci칩n *log-likelihood*, $\ell(p;\ X)$. Igualamos la
primera derivada de esa funci칩n a $0$ y despejamos $p$ para obtener as칤 la
estimaci칩n, es decir, $\hat{p}$.

A modo de pr치ctica podr칤as intentar repetir t칰 solo el proceso y comprobar que
has llegado al mismo resultado. Deja pasar unos d칤as para que se te olvide lo
que acabas de leer e int칠ntalo t칰 mismo a ver si lo sacas (estoy seguro de que
s칤 :).

Si encuentras cualquier fallo, ya sea hortogr치fico, algo mal explicado, alg칰n
n칰mero que no est치 donde tiene que estar... lo que sea, d칠jame un comentario.
Cualquier duda que tengas tambi칠n es bienvenida al tabl칩n de comentarios.

Por cierto, lo de "hortogr치fico" era broma, solo quer칤a comprobar si estabas
atento 游땦.


## Cr칠ditos

* Las im치genes de las monedas son una modificaci칩n propia de varios ficheros
descargados desde [Freepik].

* La gr치fica del logaritmo ha sido hecha con la ayuda de la librer칤a
[JSXGraph].


<!-- REFERENCES AND STYLE -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/jsxgraph/1.4.6/jsxgraphcore.js"></script>
<style>
.coin, .head, .tail {
    background-size: cover;
    display: inline-block;
    height: 2em;
    width: 2em;
    vertical-align: middle;
    margin-top: 1px;
}
.head {
    background-image: url("/assets/images/2018_10_18_coin_head.png");
}
.tail {
    background-image: url("/assets/images/2018_10_18_coin_tail.png");
}
.MathJax_Display > span.MathJax {
    overflow-y: hidden;
    overflow-x: auto;
    display: block;
    text-align: center;
    outline: 0;
}
</style>

[mle1]: /math/maximum-likelihood-estimation-1
[Freepik]: http://www.freepik.com
[JSXGraph]: http://jsxgraph.uni-bayreuth.de
